{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark's ML Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Learning Objectives:\n",
    "* Understanding Spark as an ecosystem\n",
    "* Machine Learning on Spark\n",
    "* The ML library vs MLlib vs spark-sklearn\n",
    "* Main classes in ML\n",
    "* Use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Understanding Spark as an Ecosystem\n",
    "\n",
    "* ETL\n",
    "* ML\n",
    "* Streaming\n",
    "* In-memory database (SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![sparkroadmap](images/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Machine Learning on Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![sparkroadmap](images/our-spark-roadmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Motivation:\n",
    "\n",
    "* When your data is collosal and needs a cluster\n",
    "* Because dumb models on big data are (usually) better than smart models on small data\n",
    "* When you need to retrain models regularly\n",
    "* When you're using a Spark ecosystem (ETL, stream processing, in-memory database, etc)\n",
    "* A (very) active [bazaar](https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Why Spark ML is fast\n",
    "* In memory processing\n",
    "* Smart DAG scheduling\n",
    "* TorrentBroadcast intermediate models\n",
    "* Tree aggregation\n",
    "* hashing functions\n",
    "* probabilitic data structures\n",
    "* Static vs dynamic typing (motivation for Scala over python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![torrent](images/torrent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The ML library vs MLlib vs spark-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![quote](images/quote.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Spark is moving from RDD's towards DataFrames\n",
    "* MLlib is Spark's machine learning library for RDD's\n",
    "* ML operates on DataFrames instead\n",
    "* ML is more flexible and veritile, thanks to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![comparison](images/RDD_dataframe_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "source: *High Performance Spark* early release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Where are they now?\n",
    "* MLlib is currently in maintenance\n",
    " - It is not being actively developed\n",
    "* Spark encourages developers to develop for ML instead of MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### spark-sklearn [package](https://github.com/databricks/spark-sklearn)\n",
    "\n",
    "* Scikit-learn integration package for Apache Spark\n",
    "* Distributes multi-processed components of sklearn across a cluster\n",
    " - Works well for grid search\n",
    " - Easy conversions between spark DataFrames and numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Use spark_sklearn’s grid search instead:\n",
    "from spark_sklearn import GridSearchCV\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [10, 20, 40, 80]}\n",
    "gs = grid_search.GridSearchCV(RandomForestClassifier(),\n",
    "param_grid=param_grid)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Order of Operations\n",
    "\n",
    "1. ML\n",
    "2. MLlib\n",
    "3. spark-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Main classes in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Three abstract classes in [Spark's ML library](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html):\n",
    "\n",
    "1. `Transformers`: transforms your data from one dataset to another by (normally) appending a new column to your DataFrame.\n",
    "2. `Estimators`: these are the statistical models that we estimate.  The ML library has a suite of classification, regression, and clustering algorithms.\n",
    "3. `Pipelines`: offers an end-to-end transformation-estimation process with distinct stages.  A pipeline will ingest raw data in the form of a DataFrame and perform the necessary transformations in order to estimate a statistical model.  A pipeline often consists of multiple transformers that feed into an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Takes a DataFrame, returns a transformed DF\n",
    "* Uses the `transform` method\n",
    "* Note that this normally creates new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Examples: `ChiSqSelector`, `CountVectorizer`, `Normalizer`, `OneHotEncoder`, `Tokenizer`, `StopWordsRemover`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"tweet\", outputCol=\"words\", pattern='\\s+|[,.\\\"]')\n",
    "transformed_df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Estimators that fit models to data\n",
    "* Uses the `fit` and `transform` methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Examples:\n",
    "\n",
    "| Classification | Regression | Clustering | Recommendation | \n",
    "|----|-----|------|------|\n",
    "| `LogisticRegression` | `LinearRegression` | `KMeans` | `ALS` | \n",
    "| `RandomForestClassifier` (multiclass) | `RandomForestRegressor` | `LDA`| |\n",
    "| `GBTClassifier` | `GBTRegressor` | `GaussianMixture` | |\n",
    "| `NaiveBayes` |  | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "forestizer = RandomForestClassifier(labelCol=\"lang\", featuresCol=\"features\", numTrees=10)\n",
    "model = forestizer.fit(transformed_df)\n",
    "y_hat = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Quick assembly of ML pipelines\n",
    "  - feature extraction -> dimensionality reduction -> model training\n",
    "* Uses `fit` and `transform`\n",
    "* Since transformers add columns, `inputCol` and `outputCol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import pyspark.ml.evaluation as ev\n",
    "from pyspark.ml.feature import RegexTokenizer, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"tweet\", outputCol=\"words\", pattern='\\s+|[,.\\\"]')\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "forestizer = RandomForestClassifier(labelCol=\"lang\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[\\\n",
    "                tokenizer, \n",
    "                hashingTF, \n",
    "                idf,\n",
    "                forestizer])\n",
    "\n",
    "tweets_train, tweets_test = df.randomSplit([0.7, 0.3], seed=123)\n",
    "model = pipeline.fit(tweets_train)\n",
    "test_model = model.transform(tweets_test)\n",
    "\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='lang')\n",
    "print('AUC for Random Forest:', evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderROC'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Other Fun Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Evaluation\n",
    "* Cross-validation \n",
    "* Gridsearch\n",
    "* Train/Test Splits\n",
    "* Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Recommender systems\n",
    "* Fraud/Anomoly detection\n",
    "* Deep learning\n",
    "* Genomics\n",
    "* Deep Learning CV and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Spark ML is a powerful tool for ML at scale\n",
    "* It allows for data projects built in a Spark ecosystem\n",
    "* It evolves fast.  Very fast.  Don't expect it to be future-proof\n",
    "* Looking for an open source project?  Develop for ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Other Resources\n",
    "\n",
    "* [Spark ML Documents](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html)\n",
    "* [Learning Pyspark](https://www.amazon.com/Learning-PySpark-Tomasz-Drabas/dp/1786463709)\n",
    "* [High Performance Spark (Currently early release)](http://shop.oreilly.com/product/0636920046967.do?cmp=af-strata-books-videos-product_cj_9781491943137_%25zp)\n",
    "* [Lessons for Large-Scale Machine Learning (Somewhat Outdated)](http://go.databricks.com/large-scale-machine-learning-deployments-spark-databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
