dated":"2017-04-29T10:01:51+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493457975183_-1259910826","id":"20170429-092615_655123498","dateCreated":"2017-04-29T09:26:15+0000","dateStarted":"2017-04-29T09:26:35+0000","dateFinished":"2017-04-29T09:26:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3051"},{"text":"%pyspark\n#Step 2: Preparing our Data\nimport json","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493458024159_1186784051","id":"20170429-092704_865943668","dateCreated":"2017-04-29T09:27:04+0000","dateStarted":"2017-04-29T10:02:07+0000","dateFinished":"2017-04-29T10:02:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3052"},{"text":"%pyspark\n#2. Open a Zeppelin notebook and import an hour's worth of raw tweets from s3 as an RDD\ntext_file = sc.textFile(\"s3a://twitterlabstream/YYYY/MM/DD/HH2017/04/10/21/twitterstreamlabjc*\")","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493458023931_1471498237","id":"20170429-092703_467809891","dateCreated":"2017-04-29T09:27:03+0000","dateStarted":"2017-04-29T10:02:09+0000","dateFinished":"2017-04-29T10:02:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3053"},{"text":"%pyspark\n#Return tweet (tweet text) and lang (0 for English or 1 if Spanish). Return None if not an English or Spanish tweet\ndef getVals(line):\n    tweet = json.loads(line.strip())\n    text = tweet.get('text')\n    lang = tweet.get('lang')\n    if tweet.get('id'):\n        if tweet.get('lang') == 'en':\n            return (tweet.get('text'), 0)\n        elif tweet.get('lang') == 'es':\n            return (tweet.get('text'), 1)\n    return None","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493458070335_-1986747471","id":"20170429-092750_2059506232","dateCreated":"2017-04-29T09:27:50+0000","dateStarted":"2017-04-29T10:02:10+0000","dateFinished":"2017-04-29T10:02:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3054"},{"text":"%pyspark\n#3. Transform data so there are two values: tweet (tweet text) and lang (0 if English or 1 if Spanish). Disregard all other tweets\ntweets = (text_file\n    .map(getVals)\n    .filter(lambda x: x != None)\n    )","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493458023708_1457262527","id":"20170429-092703_915688511","dateCreated":"2017-04-29T09:27:03+0000","dateStarted":"2017-04-29T10:02:13+0000","dateFinished":"2017-04-29T10:02:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3055"},{"text":"%pyspark\n#4. Create a DataFrame out of this using the toDF method and cache it\ntweetsdf = tweets.toDF(['tweet', 'lang']).cache()","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493459389602_840099577","id":"20170429-094949_1218268820","dateCreated":"2017-04-29T09:49:49+0000","dateStarted":"2017-04-29T10:02:15+0000","dateFinished":"2017-04-29T10:02:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3056"},{"text":"%pyspark\n#5. Create a train/test split with 70% of data in training set and 30% of data in test set\ntweets_train, tweets_test = tweetsdf.randomSplit([0.7, 0.3], seed=123)","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493459548511_484008153","id":"20170429-095228_2062418796","dateCreated":"2017-04-29T09:52:28+0000","dateStarted":"2017-04-29T10:02:19+0000","dateFinished":"2017-04-29T10:02:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3057"},{"text":"%pyspark\n#Step 3: Building our Pipelines\nfrom pyspark.ml.feature import RegexTokenizer, HashingTF, IDF\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\n\n\ntokenizer = RegexTokenizer(inputCol=\"tweet\", outputCol=\"words\", pattern='\\s+|[,.\\\"]')\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nforestizer = RandomForestClassifier(labelCol=\"lang\", featuresCol=\"features\", numTrees=10)\n\npipeline = Pipeline(stages=[\\\n                    tokenizer,\n                    hashingTF,\n                    idf,\n                    forestizer])\n                    \n","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493459721174_650481834","id":"20170429-095521_2011092186","dateCreated":"2017-04-29T09:55:21+0000","dateStarted":"2017-04-29T10:02:22+0000","dateFinished":"2017-04-29T10:02:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3058"},{"text":"%pyspark\n# Fit the pipeline to training documents.\nmodel = pipeline.fit(tweets_train)","user":"anonymous","dateUpdated":"2017-04-29T10:02:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1493460044667_675766347","id":"20170429-100044_1651073243","dateCreated":"2017-04-29T10:00:44+0000","dateStarted":"2017-04-29T10:02:25+0000","dateFinished":"2017-04-29T10:03:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3059"},{"text":"%pyspark\n#Step 4: Evaluating and Saving Model\nimport pyspark.ml.evaluation as ev\n\ntest_model = model.transform(tweets_test)\nevaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='lang')\nprint('AUC for Random Forest:', evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderROC'}))\n#('AUC for Random Forest:', 0.9110919373915353)","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('AUC for Random Forest:', 0.9110919373915353)\n"}]},"apps":[],"jobName":"paragraph_1493460829903_1236091562","id":"20170429-101349_116347783","dateCreated":"2017-04-29T10:13:49+0000","dateStarted":"2017-04-29T10:14:24+0000","dateFinished":"2017-04-29T10:14:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3060"},{"text":"%pyspark\n#selected = prediction.select(\"tweet\", \"lang\", \"probability\", \"prediction\")\n#for row in selected.collect():\n    #tweet, lang, prob, predlang = row\n    #print(\"(%s, %s) --> prob=%s, prediction=%d\" % (tweet, lang, str(prob), predlang))\n","user":"anonymous","dateUpdated":"2017-04-29T10:28:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1493460352287_469721538","id":"20170429-100552_727800147","dateCreated":"2017-04-29T10:05:52+0000","dateStarted":"2017-04-29T10:11:00+0000","dateFinished":"2017-04-29T10:11:10+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3061"},{"text":"","dateUpdated":"2017-04-29T06:53:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1493448814540_-99285603","id":"20170210-062804_292955085","dateCreated":"2017-04-29T06:53:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3062"}],"name":"Lab6_4__SparkML_JC_Driver_Gavin_Navigator","id":"2CG9WYWR5","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}
